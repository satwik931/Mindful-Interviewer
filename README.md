# Mindful-Interviewer

A multimodal, adaptive interview assistant that listens, watches, and responds to a candidate — combining facial emotion, voice features, and transcribed text to adapt follow-up questions and display a friendly avatar. The system is implemented in Python and is designed as a research / prototype platform to explore multimodal interviewing and supportive, mindful interviewer behavior.

- Repository: satwik931/Mindful-Interviewer
- Language: Python

---

Table of contents
- Overview & Goals
- Project structure (per-file responsibilities)
- How it works (high-level data flow)
- Requirements & installation
- Environment variables & configuration
- Quick start — run the interview
- Details & developer notes (robustness, improvements)
- Troubleshooting
- Contributing
- License

---

Overview & goals

Mindful-Interviewer aims to create a supportive, adaptive interview experience by combining:
- Visual cues (facial emotion)
- Vocal cues (pitch, energy)
- Verbal cues (transcribed text and filler words)

These signals are fused into a single sentiment estimate that the assistant uses to adapt the next question generated by a large language model (Google Gemini in this prototype). An animated avatar speaks the questions and displays simple facial states (neutral, thinking, smiling, speaking, etc.) to create a continuous, human-like interaction loop.

This repository is a prototype and research tool rather than a production-ready hiring product. It demonstrates a multimodal pipeline and highlights practical considerations (privacy, calibration, fairness).

---

Project structure and per-file responsibilities

- analyze_audio.py
  - Loads an audio file, computes RMS energy and an estimated average pitch using `librosa` and returns a dictionary:
    - `{'average_pitch_hz': <float>, 'energy': <float>}`
  - Contains a small demo that synthesizes a 440Hz sine wave file (`test_audio.wav`) and runs the analyzer.

- analyze_text.py
  - Defines `analyze_filler_words(text)` which counts common filler words/phrases in transcribed text and returns:
    - `'word_count'`, `'filler_count'`, `'filler_ratio'`
  - Includes example usage comparing a "confident" vs "nervous" sample.

- analyze_video.py
  - Uses `OpenCV` + `deepface` to capture webcam frames and analyze facial emotions in real-time.
  - Draws rectangles around detected faces and overlays the dominant emotion label on the video feed.

- analyze_display.py
  - Loads and displays a PNG avatar image corresponding to a named emotion (e.g., `avatar_images/smiling.png`).
  - Waits for a configurable duration, and falls back to `avatar_images/neutral.png` if the requested image can't be loaded.

- core_logic.py
  - Takes modality analyses (face, voice, text) and:
    - Maps facial emotion labels to numeric scores.
    - Converts voice pitch and text filler ratio to numeric sub-scores.
    - Combines them via weighted averaging into a fused sentiment score in [-1.0, 1.0].
  - Uses the fused score and the conversation history to prompt Google Gemini to generate the next interview question (the prompt requests a JSON response with `question_text` and `suggested_avatar_emotion`).

- main.py
  - Orchestrates the interview loop:
    1. Initializes webcam, microphone, and UI window.
    2. Seeds the conversation with an initial greeting.
    3. Repeatedly:
       - Speaks the LLM-generated question with `speak_with_animation`.
       - Listens (SpeechRecognition) for the candidate's response.
       - Captures a frame for facial analysis.
       - Saves audio to a temporary file and runs `analyze_audio` and `analyze_text`.
       - Fuses signals with `core_logic.normalize_and_fuse_sentiment`.
       - Requests the next question from the LLM with `core_logic.generate_adaptive_question`.
       - Appends utterances to the conversation history and continues until an exit phrase is detected.
    4. Says a final goodbye and cleans up resources.

- output_engine.py
  - `speak_with_animation(text, emotion, audio_file="response.mp3")`:
    - Uses `gTTS` to synthesize speech to an MP3 file.
    - Plays audio via `pygame.mixer` while displaying a "speaking" avatar image using OpenCV.
    - Reverts to the neutral avatar and removes the temporary audio file when done.

Additional assets (expected)
- `avatar_images/` directory with images named at least:
  - `neutral.png`, `speaking.png`, `smiling.png`, `thinking.png`, `encouraging_nod.png`
- A `.env` file containing `GOOGLE_API_KEY` (for the Gemini calls via `google.generativeai`) if you plan to use Google Gemini.

---

How it works — high level data flow

1. The interviewer (AI) speaks a question (TTS) and animates the avatar.
2. The candidate answers verbally; the app records audio (SpeechRecognition).
3. The system captures a webcam frame and runs facial emotion analysis (DeepFace).
4. The recorded audio is transcribed (Google STT via SpeechRecognition) and saved to a temporary WAV for audio feature extraction (librosa).
5. `analyze_audio.py` extracts energy and pitch; `analyze_text.py` counts filler words.
6. `core_logic.py` converts per-modality outputs into numerical scores and fuses them into a single sentiment score.
7. The conversation history and fused sentiment are used to prompt an LLM (Gemini) to generate the next interview question and suggested avatar emotion.
8. The loop repeats.

---

Requirements & installation

Minimum environment
- Python 3.8+ (3.10 or 3.11 recommended)
- A system with a working webcam, microphone, and speakers for full functionality.

Example: create and activate a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate    # macOS / Linux
.venv\Scripts\activate       # Windows
```

Install dependencies
- If the repository includes a `requirements.txt`, use:
```bash
pip install -r requirements.txt
```

Example dependencies (iterate and pin versions in real project)
```bash
pip install numpy librosa soundfile opencv-python deepface SpeechRecognition pyaudio gTTS pygame mutagen python-dotenv google-generativeai
```

Notes:
- On Windows, installing PyAudio is sometimes easier via `pipwin`:
  ```bash
  pip install pipwin
  pipwin install pyaudio
  ```
- DeepFace may require additional system packages (for certain backends). See DeepFace docs.
- `librosa` requires `soundfile`/libsndfile; on some platforms you may need to install system packages (e.g., `apt-get install libsndfile1`).
- `pygame` may need SDL dependencies installed by your OS package manager for best compatibility.
- `gTTS` and `google.generativeai` require network access.
- For robust MP3/WAV handling and conversions, having `ffmpeg` on your PATH is recommended.

Create a `.env` file
```
GOOGLE_API_KEY=your_google_gemini_api_key_here
```
- Keep `.env` secret. Do not commit API keys to git.

---

Quick start / Example run

1. Make sure `avatar_images/` contains the required PNG files (`neutral.png`, `speaking.png`, `smiling.png`, `thinking.png`, `encouraging_nod.png`).
2. Activate virtual environment and install dependencies (as above).
3. Create `.env` with `GOOGLE_API_KEY` or set `GOOGLE_API_KEY` in your environment.
4. Run the main program:
```bash
python main.py
```

Behavior:
- The AI will greet you and ask the first question. It will speak, listen, analyze face/voice/text, and adapt the next question. Say "goodbye" or "end interview" to stop.

---

Developer notes, limitations, and improvement ideas

Important caveats
- This is a research/prototype demo. The mapping from multimodal signals to a single "sentiment" is heuristic and must be validated before any real-world use, especially in hiring contexts.
- Facial and vocal cues are culturally and individually dependent. Models can be biased; treat outputs as cues, not ground truth decisions.
- The system sends data (conversation history and possibly audio-derived content) to cloud APIs (Google STT, Gemini). Ensure you have consent and handle privacy appropriately.
- The Gemini prompt expects a JSON response but LLMs may sometimes deviate. The code attempts to parse model output as JSON; you should include robust fallbacks and sanitization in production.

Suggested improvements
- Robust pitch estimation: replace or augment `librosa.piptrack` with `librosa.yin`/`pyin`, median/voiced-frame aggregation, voiced/unvoiced ratio.
- Better filler detection: use tokenization + regex for multi-word fillers, handle punctuation and repeated characters (e.g., "ummmm").
- Smooth face estimates: aggregate emotion results over several frames and smooth with an exponential moving average for stable suggestions.
- Non-blocking UI: move heavy operations (TTS, LLM calls, DeepFace) to worker threads so the UI remains responsive and avatar animations run smoothly.
- Replace gTTS (cloud dependency) with an offline TTS for low-latency demos (e.g., pyttsx3 or Coqui TTS).
- Use structured model outputs / schema features if available from the LLM SDK to avoid fragile JSON parsing.
- Centralize configuration: weights, thresholds, and emotion-score mapping should be parameterized (YAML, JSON, or environment variables) for easy experimentation.
- Add unit tests for core logic and examples for reproducible evaluation.

Security & privacy checklist
- Do not commit `.env` or keys to the public repository.
- Inform participants their audio and video are recorded and may be processed by cloud services.
- Consider on-device alternatives for transcription / TTS if privacy is required.

---

Troubleshooting & common issues

- Webcam not opening:
  - Ensure no other app is using the camera. On macOS check System Preferences → Security & Privacy → Camera to allow terminal access.
  - Try a different device index: `cv2.VideoCapture(1)`.

- Microphone errors / PyAudio:
  - On Windows, install PyAudio using `pipwin`.
  - Confirm microphone is accessible and not used by another application.

- DeepFace errors:
  - Ensure required model weights are available; DeepFace will download models on first run (needs internet).
  - If analysis fails often, consider catching exceptions and fallback to `'neutral'` as the code currently does.

- gTTS / network timeout:
  - gTTS requires internet. Use offline TTS for disconnected environments.

- LLM errors:
  - Ensure `GOOGLE_API_KEY` is set and valid.
  - Truncate or summarize very long `conversation_history` before sending to avoid token limits.
  - Add retries with exponential backoff for transient network/API errors.

---

Testing & demos

- Several modules include small demo code blocks (`if __name__ == '__main__'`) to run local tests quickly:
  - `analyze_audio.py` — creates a synthetic wav and runs analyzer.
  - `analyze_text.py` — compares two short sample strings.
  - `output_engine.py` — plays a test TTS sample.

---

Acknowledgements & resources

- DeepFace — facial analysis library
- librosa — audio analysis and pitch estimation
- SpeechRecognition — microphone capture and Google STT helper
- gTTS — Google Text-to-Speech wrapper
- pygame — simple audio playback for TTS
- OpenCV — webcam capture and image display
- Google Gemini (Generative AI) — LLM question generation (via `google.generativeai`)

---

